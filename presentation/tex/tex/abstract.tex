\begin{abstract}

Researchers from industry and academia have recently proposed to disaggregate
memory in warehouse-scale computers, motivated by the increasing performance of
networks, and a proliferation of novel memory technologies. In a system with
memory disaggregation, each compute node contains a modest amount of fast
memory (e.g.  high-bandwidth DRAM integrated on-package), while large capacity
memory or non-volatile memory is made available across the network through
dedicated memory nodes. One common proposal to harness the fast local memory is
to use it as a large cache for the remote bulk memory. This cache could be
implemented purely in hardware, which could minimize latency, but may involve
complicated architectural changes and would lack OS insights into memory usage.
An alternative is to manage the cache purely in software with traditional
paging mechanisms. This approach requires no additional hardware, can use
sophisticated algorithms, and has insight into memory usage patterns. However,
our experiments show that even when paging to local memory, applications can be
slowed significantly due to the overhead of handling page faults, which can
take several microseconds and pollute the caches. In this paper, we present an
extension to the memory management unit that partially automates page faults,
removing the OS from the latency-critical page-in pathway. With this
accelerator, applications on a disaggregated system spend up to 2.5x less time
managing paging, and run up to 40\% faster end-to-end.

\end{abstract}

