Traditional data center design aggregates all necessary resources (e.g., disk,
memory, power supply, etc.) into many self contained server chassis. This
design was motivated by the ability to leverage commodity PC components and
networks\cite{NOW}. Additionally, an aggregated design was desirable because
in-chassis interconnects were significantly faster than networks. However, data
center-side compute has grown into an important independent market, leading to
specialized server platforms and networks (often called \glspl{wsc}).
Furthermore, networking technology has seen a rapid increase in performance,
with \SI{40}{\giga\bit\per\second} Ethernet becoming commonplace, and 100+
\si{\giga\bit\per\second} networks readily available, narrowing the gap between
off-package DRAM and remote memory.  Workloads have also changed; applications
are fundamentally distributed (e.g., service-oriented architecture, map-reduce,
etc.), use larger and rapidly changing datasets (``Big Data''), and demand
latencies that can only be delivered by in-memory processing. Finally, a number
of promising new memory technologies are becoming available. New \gls{nvm}
devices are being introduced that promise low idle power, high density, and
near-DRAM performance (e.g., fast NAND, phase-change, memristor). On the
high-performance side, improvements in packaging technology have led to fast
on-package DRAM (e.g., HBM) that offers hundreds of GB/s of bandwidth with
capacities in the tens of GB.

These hardware and software trends have lead to proposals from both
academia\cite{firebox}\cite{dredbox} and
industry\cite{themachine}\cite{huaweidc30}\cite{intelrsa}\cite{fbdisag} for a
new style of \gls{wsc} where resources are disaggregated \glsadd{disag}. In a
disaggregated \gls{wsc}, resources like disk and memory become first-class
citizens over a high-performance network. A compute node couples CPUs, network
interfaces, and a small amount of high-speed memory into a self-contained
\gls{sip}. This design allows data center operators to scale memory capacity,
while allocating it more flexibly (avoiding stranded resources and complex
resource allocation policies). However, the memory access latency will be
higher than traditional off-package DRAM, and bandwidth may be limited or
subjected to congestion. The small on-package memory allows us to mitigate some
of this performance gap, but the question remains: how best to use it?

One way to harness the on-package DRAM is to use it as a large cache for remote
bulk memory. Operating systems have traditionally provided this through virtual
memory \gls{paging} which uses virtual memory to treat local physical memory as
a software-managed cache (typically for disk). Indeed, several recent academic
research projects have proposed using paging over \gls{rdma} as a way of
disaggregating memory\cite{infiniswap}\cite{osdidisag}. Paging has
traditionally been backed by slow disks with access latencies in the
milliseconds. This lead to sophisticated algorithms that can take several
microseconds for every cache miss. An alternative is to have fully hardware
managed DRAM caches\cite{volos_DRAM}\cite{lee_tagless}. These eliminate much of
the overhead, but lack the sophistication and application-level insight of
OS-based approaches. For example, operating systems often use significant
memory for optimistic pre-fetching and caching of disk blocks. A
hardware-managed cache may choose to store these in remote memory, while the OS
would simply delete them.

This paper introduces a hardware accelerator for OS-managed caching called the
\gls{pfa}. The \gls{pfa} works by handling latency-critical page faults
(cache-miss) in hardware, while allowing the OS to manage latency-insensitive
(but algorithmically complex) evictions asynchronously. We achieve this
decoupling with a queue of free page frames (freeQ) to be used by the \gls{pfa}
for fetched pages, and a queue of new page descriptors (newQ) that the OS can
use to manage new page meta-data. Execution then proceeds as follows:

\begin{itemize}
	 \item The OS allocates several page frames and pushes their addresses onto
		 the \emph{freeQ}.
   \item The OS experiences memory pressure and selects pages to evict to
		 remote memory. It marks them as ``remote'' in the page tables and then
     provides them to the \gls{pfa} for eviction.
   \item The application attempts to access a remote page, triggering the
     \gls{pfa} to request the page from remote memory and place it in the next
     available free frame. The application is then resumed.
	 \item Some time later (either through a background daemon, or through an
		 interrupt due to full queues) the OS pops all new page descriptors off
		 the \emph{newQ} and records the (now local) pages in its meta-data. The
		 OS typically provides more free frames at this time.
\end{itemize}

